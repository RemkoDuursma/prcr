# Classification

## Introduction

We have now explored various basic and more advanced regression approaches - each time we were trying to explain variation in a *continuous* (numeric) variable. In classification, we are instead trying to explain a *discrete* (factor) variable. There are many practical applications of classification, and R has many tools to fit, analyze, and visualize classification models.

In the previous chapter, we looked in detail at logistic regression models. These models aim to predict the *probability* of some occurrence (for example, the probability of dying when you travelled on the Titanic's ultimate journey). What if we want to convert a *probability* (a number between 0 and 1) to a *classifier* (either 0 *or* 1)? This is a harder problem than it may seem at first sight, and we will discuss it in the next section.

After we know how to construct a classifier, we will look at a few different popular classification algorithms, without going into much of the theory underlying them. Instead, we will treat the topic by example.



**Packages used in this chapter**

```{r include=FALSE, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(ranger))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggthemes))
suppressPackageStartupMessages(library(lgrdata))
suppressPackageStartupMessages(library(rpart.plot))
```

- `caret` : for various machine learning helper functions
- `rpart` : recursive partitioning
- `RandomForest` : for Random Forest
- `ranger` : for Random Forest (faster!)



## From probabilities to classification

threshold
confusion matrix




```{r}
library(lgrdata)
data(titanic)


library(rpart)
titanic$Survived <- as.factor(titanic$Survived)
r1_titanic <- rpart(Survived ~ PClass + Age + Sex, data=titanic)

```

```{block2 type="rmdtry"}
Look at `summary(r1_titanic)` for a detailed explanation of the fitted recursive partitioning object. Usually, a plot is more insightful.
```



```{r}
library(rpart.plot)
prp(r1_titanic,
    clip.right.labs = FALSE)
```


```{r}
# this seems like a nice option
# box:
# - 0 or 1 (predicted class on this leaf)
# - classification rate (correct / n)
# - % of all observations fall into this leaf
xy <- rpart.plot(r1_titanic, type=3, extra=102)
```



```{r}
library(caret)
confusionMatrix(predict(r1_titanic, type="class"),
                       titanic$Survived,
                       positive = "1")
```




Recursive partitioning is a divisive clustering method. The tree is grown from the top, deciding each time on the optimal split in two classes. The process is repeated for each of the groups, all the way down. A regression or classification tree constructed in this way will be very much overfitted; in other words it leads to a model that is useless to predict new cases. 

* the tree is pruned back based on cross validation errors

For a classification tree, we do

```{r}
data(mushroom)

library(rpart)
rp1 <- rpart(edibility ~ ., data=mushroom, method="class")
```

Confusion matrix

```{r}
table(predict(rp1, type="class"), mushroom$edibility)
```

Each split in the tree is based on maximizing the inequality between the two groups (the basic idea of clustering!). We can decide on either the *information* or the Gini coefficient. This setting is very well hidden in `?rpart`.


To use the Gini coefficient instead of the Information to decide the splits (which is the default), set it like this:

```{r}
rp3 <- rpart(edibility ~ ., data=mushroom, method="class", parms=list(split="gini"))
```

In this case, the results are exactly the same - but this won't always be the case.




# Multiple classes

```{r eval=FALSE}
library(dplyr)
data(cereals)
cereals2 <- filter(cereals, 
                   Manufacturer != "A",
                   !is.na(carbo),
                   !is.na(potass),
                   !is.na(sugars)
                   ) %>%
  droplevels


form <- formula(Manufacturer ~ calories + protein + fat + sodium + fiber + carbo + sugars + potass + vitamins)

library(rpart)
c1 <- rpart(form, data=cereals2, method="class")
```



```{r}
predict(c1, type = "class")

mean(predict(c1, type = "class") == cereals2$Manufacturer)

```



```{r}
library(caret)
confusionMatrix(predict(c1, type = "class"), 
                       cereals2$Manufacturer)$table


sum(predict(c1) == cereals2$Manufacturer) / nrow(cereals2)


```


```{r}
tab_confus <- confusionMatrix(predict(c1, type = "class"), 
                       cereals2$Manufacturer)$table

library(reshape2)
dfr_confus <- melt(tab_confus)


library(ggplot2)

ggplot(dfr_confus, aes(x = Reference, y = Prediction, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue")

```

```{r}
filter(dfr_confus, Reference == Prediction) %>% pull(value) %>% sum
```



## Random Forest

```{r}

library(randomForest)


c2 <- randomForest(form, data=cereals2, mtry = 3, na.action = na.omit)
confusionMatrix(predict(c2), 
                       cereals2$Manufacturer)

# Accuracy
sum(predict(c2) == cereals2$Manufacturer) / nrow(cereals2)



library(e1071)

c3 <- naiveBayes(form, data=cereals2)
confusionMatrix(predict(c3, newdata = cereals2), 
                       cereals2$Manufacturer)


```



## Visualizing classifiers


```{r eval=FALSE}


library(ggplot2)
library(ggthemes)


ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, col=Species)) +
  geom_point(size=3) +
  theme_tufte() +
  scale_colour_tableau() +
  lims(x=c(4,8), y=c(2,4.5))


```




```{eval = FALSE}
class_predictions_plot <- function(model, n=101, fun=function(x)x, add_points=TRUE){
  
  out <- expand.grid(Sepal.Length = seq(4,8, length=n),
                     Sepal.Width = seq(2, 4.5, length=n))
  
  pred <- predict(model, newdata = out)
  out$pred_species <-  fun(pred)

  g <- ggplot(out, aes(x = Sepal.Length, y = Sepal.Width)) +
    geom_tile(aes(fill=pred_species)) +
    scale_fill_economist() +
    theme_tufte()
  
  if(add_points){
    g <- g + 
      geom_point(data = iris, 
                 aes(x = Sepal.Length, y = Sepal.Width, fill=Species),
                 pch = 21, size=2.5) 
  }
  
print(g)
}


library(MASS)
lda1 <- lda(Species ~ Sepal.Length + Sepal.Width, data=iris)

class_predictions_plot(lda1, fun = function(x)x$class)




library(randomForest)
rf1 <- randomForest(Species ~ Sepal.Length + Sepal.Width, data=iris)

class_predictions_plot(rf1)



library(rpart)
rp1 <- rpart(Species ~ Sepal.Length + Sepal.Width, data=iris)
class_predictions_plot(rp1)




library(caret)

rp1 <- train(Species ~ Sepal.Length + Sepal.Width, data=iris, method="rpart")
class_predictions_plot(rp1)


# 
m1 <- train(Species ~ Sepal.Length + Sepal.Width, data=iris, method="multinom")
class_predictions_plot(m1)

```



```{r eval=FALSE}
library(e1071)

s1 <- svm(Species ~ Sepal.Length + Sepal.Width, data=iris, kernel = "radial")
class_predictions_plot(s1)

s2 <- svm(Species ~ Sepal.Length + Sepal.Width, data=iris, kernel = "linear")
class_predictions_plot(s2)

s3 <- svm(Species ~ Sepal.Length + Sepal.Width, data=iris, kernel = "polynomial")
class_predictions_plot(s3)

s4 <- svm(Species ~ Sepal.Length + Sepal.Width, data=iris, kernel = "sigmoid")
class_predictions_plot(s4)


```

