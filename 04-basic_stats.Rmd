# Basic statistics {#basic}

```{r echo=FALSE, message=FALSE}
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(Hmisc))
suppressPackageStartupMessages(library(lgrdata))
suppressPackageStartupMessages(library(moments))
suppressPackageStartupMessages(library(pastecs))

```


This book is not an *Introduction to statistics*. There are many books focused on *statistics*, with or without example R code. The focus throughout this book is on the R code itself, as we try to present clear and short solutions to common analysis tasks. In the following, we assume you have a basic understanding of linear regression, Student's $t$-tests, ANOVA, and confidence intervals for the mean.


## Probability distributions {#distributions}

You will have encountered a number of probability distributions before. For example, the *Binomial* distribution is a model for the distribution of the number of *successes* in a sequence of independent trials, for example, the number of heads in a coin tossing experiment. Another commonly used discrete distribution is the *Poisson*, which is a useful model for many kinds of count data. Of course, the most important distribution of all is the *Normal* or Gaussian distribution.

R provides sets of functions to find densities, cumulative probabilities, quantiles, and to draw random numbers from many important distributions. The names of the functions all consist of a one letter prefix that specifies the type of function and a stem which specifies the distribution. Look at the examples in the table below.

**Prefix**

```{r echo=FALSE}
library(tibble)

tab <- tribble(~Prefix, ~Meaning,
               "`d`", "density",
               "`p`", "cumulative probability",
               "`q`", "quantile",
               "`r`", "simulate")

knitr::kable(tab)
```

**Suffix**

```{r, echo=FALSE}
tab <- tribble(~Suffix, ~Meaning,
               "`binom`","Binomial",
                "`pois`", "Poisson",
                "`norm`", "Normal",
                "`t`", "Student's t",
                "`chisq`", "Chi-squared",
                "`f`", "F")
knitr::kable(tab)
```

Using the prefix and the suffix, we can construct each desired function. For example, 

```{r }
# Calculate the probability of 3 heads out of 10 tosses of a fair coin.
# This is a (d)ensity of a (binom)ial distribution.
dbinom(3, 10, 0.5)

# Calculate the probability that a normal random variable (with 
# mean of 3 and standard deviation of 2) is less than or equal to 4.
# This is a cumulative (p)robability of a (norm)al variable.
pnorm(4, 3, 2)

# Find the t-value that corresponds to a 2.5% right-hand tail probability
# with 5 degrees of freedom.
# This is a (q)uantile of a (t)distribution.
qt(0.975, 5)

# Simulate 5 Poisson random variables with a mean of 3. 
# This is a set of (r)andom numbers from a (pois)son distribution.
rpois(5, 3)
```

See the help page `?Distributions` for more details.

To make a quick plot of a distribution, we can use the density function in combination with `curve`. The following code makes Fig. \@ref(fig:distplot).

```{r distplot, fig.cap='Two univariate distributions plotted with curve()', opts.label="smallsquare"}
# A standard normal distribution
curve(dnorm(x, sd=1, mean=0), from=-3, to=3,
      ylab="Density", col="blue")

# Add a t-distribution with 3 degrees of freedom.
curve(dt(x, df=3), from =-3, to=3, add=TRUE, col="red")

# Add a legend (with a few options, see ?legend)
legend("topleft", c("Standard normal","t-distribution, df=3"), lty=1, col=c("blue","red"),
       bty='n', cex=0.8)
```

```{block2 type="rmdtry"}
Make a histogram (recall Section~\ref{sec:hist}) of a sample of random numbers from a distribution of your choice.
```



## Descriptive Statistics {#descstat}

Descriptive statistics summarise some of the properties of a given data set. Generally, we are interested in measures of location (central tendency, such as mean and median) and scale (variance or standard deviation). Other descriptions can include the sample size, the range, and so on. We already encountered a number of functions that can be used to summarize a vector. 

Let's look at some examples for the Pupae dataset (described in Section~\ref{sec:pupaedata}).
  
```{r }
# Read data
data(pupae)

# Extract the weights (for convenience)
weight <- pupae$PupalWeight

# Find the number of observations
length(weight)

# Find the average (mean) weight
mean(weight)

# Find the Variance
var(weight)
```

Note that R will compute the sample variance (not the population variance). The standard deviation can be calculated as the square root of the variance, or use the `sd` function directly.

```{r }
# Standard Deviation
sqrt(var(weight))

# Standard Deviation
sd(weight)
```

Robust measures of the location and scale are the median and inter-quartile range; R has functions for these.

```{r }
# median and inter-quartile range
median(weight)
IQR(weight)
```

The median is the 50th percentile or the second quartile. The `quantile` function can compute quartiles as well as arbitrary percentiles/quantiles.

```{r }
# Default: computes quartiles.
quantile(weight)

# Or set any quantiles
quantile(weight, probs=seq(0,1,0.1))
```

**Missing Values**: All of the above functions will return `NA` if the data contains *any* missing values. However, they also provide an option to remove missing values (`NA`s) before their computations (see also Section \@ref(workingmissing)).

```{r }
mean(pupae$Frass)
mean(pupae$Frass, na.rm=TRUE)
```

The `summary` function provides a lot of the above information in a single command:
```{r }
summary(weight)
```

The `moments` package provides 'higher moments' if required, for example, 
the `skewness` and `kurtosis`.
```{r }
# load the moments package
library(moments)
skewness(weight)
kurtosis(weight)
```

The `pastecs` package includes a useful function that calculates many descriptive statistics for numeric vectors, including the standard error for the mean (for which R has no built-in function).

```{r }
library(pastecs)

# see ?stat.desc for description of the abbreviations
stat.desc(weight)

# conveniently, the output is a character vector which we can index by name,
# for example extracting the standard error for the mean
stat.desc(weight)["SE.mean"]
```


Sometimes you may wish to calculate descriptive statistics for subgroups in the data. We will come back to this extensively in Section \@ref(tapplyaggregate) and later sections.


## Inference for a single population {#inference}

*Inference* is answering questions about population parameters based on a sample. The mean of a random sample from a population is an estimate of the population mean. Since it is a single number it is called a point estimate. It is often desirable to estimate a range within which the population parameter lies with high probability. This is called a confidence interval.

One way to get confidence intervals in R is to use the quantile functions for the relevant distribution. Remember from your introductory statistics course that a $100(1-\alpha)$\% confidence interval for the mean on normal population is given by,

\[\bar{x} \pm t_{\alpha/2, n-1} \frac{s}{\sqrt{n}}\]

where $\bar{x}$ is the sample mean, $s$ the sample standard deviation and $n$ is the sample size. $t_{\alpha/2, n-1}$ is the $\alpha/2$ tail point of a $t$-distribution on $n-1$ degrees of freedom. That is, if $T$ has a $t$-distribution on $n-1$ degrees of freedom.

\[P(T \leq t_{\alpha/2, n-1}) = 1-\alpha/2 \]

The R code for this confidence interval can be written as,

```{r }
alpha <- 0.05 # for a 95% confidence interval
xbar <- mean(weight)
s <- sd(weight)
n <- length(weight)
half.width <- qt(1-alpha/2, n-1)*s/sqrt(n)

# Confidence Interval 
c(xbar - half.width, xbar + half.width)
```

Here, we assumed a normal distribution for the population. You may have been taught that if $n$ is *large*, say $n>30$, then you can use a normal approximation. That is, replace `qt(1-alpha/2, n-1)` with `qnorm(1-alpha/2)`, but there is no need, R can use the $t$-distribution for any $n$ (and the results will be the same, as the $t$-distribution converges to a normal distribution when the df is large).



### Hypothesis testing

There may be a reason to ask whether a dataset is consistent with a certain mean. For example, are the pupae weights consistent with a population mean of 0.29? For normal populations, we can use Student's $t$-test, available in R as the `t.test` function. Let's test the null hypothesis that the population mean is 0.29:

```{r }
t.test(weight, mu=0.29)
```

Note that we get the $t$-statistic, degrees of freedom ($n-1$) and a p-value for the test, with the specified alternative hypothesis (not equal, i.e. two-sided). In addition, `t.test` gives us a 95% confidence interval (compare to the above), and the estimated mean, $\bar{x}$.

We can use `t.test` to get any confidence interval, and/or to do one-sided tests,

```{r }
t.test(weight, mu=0.29, alternative="greater", conf.level=0.90)
```

Note that the confidence interval is one-sided when the test is one-sided.

The `t.test` is appropriate for data that is approximately normally distributed. You can check this using a histogram or a QQ-plot (see Sections~\ref{sec:hist} and~\ref{sec:diagplots}). If the data is not very close to a normal distribution then the `t.test` is often still appropriate, as long as the sample is large.

If the data is not normal and the sample size is small, there are a couple of alternatives: transform the data (often a log transform is enough) or use a *nonparametric* test, in this case the Wilcoxon signed rank test. We can use the `wilcox.test` function for the latter, its interface is similar to `t.test` and it tests the hypothesis that the data is symmetric about the hypothesized population mean. For example,

```{r }
wilcox.test(weight, mu=0.29)

# Likewise: (result not shown)
#wilcox.test(weight, mu=0.29, alternative="greater")
```

#### Test for proportions

Sometimes you want to test whether observed proportions are consistent with a hypothesized population proportion. For example, consider a coin tossing experiment where you want to test the hypothesis that you have a fair coin (one with an equal probability of landing heads or tails). In your experiment, you get  60 heads out of 100 coin tosses. Do you have a fair coin? We can use the `prop.test` function:

```{r }
# 60 'successes' out of a 100 trials, the hypothesized probability is 0.5.
prop.test(x=60, n=100, p=0.5)

# Same as above, but for a one-sided test.
prop.test(60, 100, p=0.5, alternative="greater")
```


## Inference for two populations

Commonly, we wish to compare two (or more) populations. For example, the `pupae` dataset has pupal weights for female and male pupae. We may wish to compare the weights of males (`gender=0`) and females (`gender=1`). 

There are two ways to use `t.test` to compare the pupal weights of males and females. In the first method, we make two vectors, 

```{r }
weight <- pupae$PupalWeight
gender <- pupae$Gender
weight.male <- weight[gender==0]
weight.female <- weight[gender==1]

# We will assume equal variance for male and female pupae (see Unequal variances, below):
t.test(weight.male, weight.female, var.equal=TRUE)
```

```{block2 type="rmdtry"}
Confirm that there are missing data in both variables in the example above. The default action is to omit all missing values (see description under `na.action` in the help file `?t.test`).
```

There is also a *formula* interface for `t.test`. The formula interface is important because we will use it in many other functions, like linear regression and linear modelling. For the `t.test` we can use the formula interface on the extracted variables, or without extracting the variables.

```{r }
# Using the vectors we constructed in the previous example
t.test(weight ~ gender,  var.equal=TRUE)

# Or by specifying the data= argument. (same result not shown)
# t.test(PupalWeight~Gender,  data=pupae, var.equal=TRUE)
```


### Paired data

The `t.test` can also be used when the data are paired, for example, measurements taken before and after some treatment on the same subjects. The `pulse` dataset is an example of paired data (see Section~\ref{sec:pulsedata}). We will compare pulse rates before and after exercise, including only those subjects that exercised (`Ran=1`),

```{r }
data(pulse)
pulse.before <- with(pulse, Pulse1[Ran==1])
pulse.after <- with(pulse, Pulse2[Ran==1])
t.test(pulse.after, pulse.before, paired=TRUE)
```

### Unequal variances

The default for the two-sample `t.test` is actually to *not* assume equal variances. The theory for this kind of test is quite complex, and the resulting $t$-test is now only approximate, with an adjustment called the 'Satterthwaite' or 'Welch' approximation made to the degrees of freedom.

Since this modified $t$-test makes fewer assumptions, you could ask why we ever use the equal variances form. If the assumption is reasonable, then this (equal variances) form will have more power, i.e. will reject the null hypothesis more often when it is actually false.

### Assumed normality

The two-sample $t$-test assumes normality of the data (which you can check using a histogram or a QQ-plot) or that the sample sizes are large enough that the *central limit theorem* applies. Note that the paired $t$-test assumes only that the differences are normal - the data themselves can still follow any number of distributions. The `wilcox.test` can be used when any of these assumptions are suspect. In the case of two samples (unpaired), this test used is called the Wilcoxon rank sum test (also known as the Mann-Whitney test).

```{r }
wilcox.test(PupalWeight ~ Gender,  data=pupae, exact=FALSE)
```

### Power {#power}

When testing a hypothesis, remember that there are two types of possible errors, due to the random nature of sampling data. These are the "Type 1 error" (rejecting the null hypothesis when it is actually true), and the "Type 2 error" (failing to reject the null when it is actually false). The probability of a Type 1 error is controlled by $\alpha$, the threshold on the $p$-value. The $p$-value is the probability of observing the test statistic, if the null hypothesis is actually true. So by keeping $\alpha$ small (for some reason, 0.05 is most commonly used), we control the chance of a Type 1 error. 

Statistical power is defined as 1 - the probability of a Type 2 error. Or in other words, the probability that we reject the null hypothesis when it is actually false. Consider the situation where we compare the means of two samples. It is easy to see that our power depends not only on $\alpha$, but also on the actual difference in means of the populations that the samples were drawn from. If they are very different, it will be easier to find a significant difference. So, to calculate the power we must specify how different the means are under the alternative hypothesis.

For a $t$-test, we can use the `power.t.test` function to calculate the power. To approximate the power for the pupal weight t-test (as we saw in the previous section), we can use the following,

```{r }
power.t.test(n=35, delta=0.08, sd=0.05, sig.level=0.05)
```

Here we have assumed equal groups of size 35 for each gender (although this is not exactly correct), a true difference in mean weights of 0.08, and a standard deviation of 0.05. The power is over 99\%, meaning that, with these conditions, we will be able to reject the null hypothesis 99\% of the time.

We can also calculate the required sample size, if we wish to attain a certain power. For example, suppose we want to detect a difference of 0.02 with 75% power. What sample size do we need?

```{r }
power.t.test(delta=0.02, sd=0.05, sig.level=0.05, power=0.75)
```

We would need 88 observations for each gender.

```{block2 type="rmdtry"}
Using `power.t.test` as in the examples above, see what happens when you set $\alpha$ (`sig.level`) to 0.01 or 0.1. Decide for yourself if the result makes sense.
```



## Simple linear regression {#simpleregression}

To fit linear models of varying complexity, we can use the `lm` function. In Chapter \@ref(linmodel) we will meet various more complex linear models, but here we just focus on simple relationships between two continuous variables. The simplest model is a straight-line relationship between an *x* and a *y* variable. In this situation, the assumption is that the *y*-variable (the response) is a linear function of the *x*-variable (the predictor, or independent variable), plus some random noise or measurement error. For the simplest case, both *x* and *y* are assumed to be continuous variables. In statistical notation we write this as,

$$
y = \alpha+\beta x +\varepsilon {#eqn:simplelin}
$$

Here $\alpha$ and $\beta$ are (population) parameters that need to be estimated from the data. The error ($\epsilon$) is assumed to follow a normal distribution with a mean of zero, and a standard deviation of $\sigma$. It is also assumed that $\sigma$ is constant and does not depend on *x*.

Let's look at an example using the allometry data (see Fig. \@ref(fig:allomquickplot)),

```{r allomquickplot, fig.cap='Quick inspection of the allometry data, before we perform a linear regression.', opts.label="smallsquare"}
# Read data
data(allometry)
plot(leafarea~diameter, data=allom)
```

We can see from this plot that leaf area generally increases with tree diameter. So we can use `lm`
to estimate the parameters in equation~\ref{eqn:simplelin}, or in other words to 'fit the model'.

```{r }
# Fit linear regression of 'leafarea' on 'diameter',
# Results are stored in an object called model
model <- lm(leafarea~diameter, data=allometry)

# Print a summary of the regression:
summary(model)

# Or just the coefficients (intercept and slope):
coef(model)
```

As you can see, `lm` uses the formula interface that we discussed earlier (it always has the form y ~ x). 

The `summary` function prints a lot of information about the fit. In this case, it shows that the intercept is -`r round(coef(model)[1],3)*-1`, which is the predicted leaf area for a tree with diameter of zero (not very useful in this case). 

It also shows a standard error and a t-statistic for this intercept, along with a p-value which shows that the intercept is significantly different from zero (because the p-value is small). The second line in the coefficients table shows the slope is `r round(coef(model)[2],3)`, and that this slope is highly significantly different from zero.

In addition, we have a `Residual standard error` of `r round(summary(model)$sigma,2)`, which is an estimate of $\sigma$, and an `R-squared` of `r round(summary(model)$r.squared,2)` (which is the squared correlation coefficient). Finally, the `F-statistic` says whether the overall fit is significant, which in this case, is the same as the test for $\beta$ (because in this situation, the F-statistic is simply the square of the t-statistic).

### Adding regression lines to a plot

It is straightforward to add the regression line to an existing plot (Fig. \@ref(fig:allomquickplot2)). Simply use `abline` and the `model` object we created previously, if you are using base graphics.

```{r allomquickplot2, fig.cap='The allometry data, with an added regression line.', opts.label="smallsquare"}
plot(leafarea~diameter, data=allom)
abline(model)
```

With `ggplot2`, we can use the handy shortcut `stat_smooth`, though the disadvantage is that it is a bit harder to control how we fit the model. For simple applications as these, though, it works fine. Results of the following example are not shown.

```{r eval=FALSE}
library(ggplot2)

ggplot(allometry, aes(x = diameter, y = leafarea)) +
  geom_point() +
  stat_smooth(method = "lm")
```



### Diagnostic plots {#diagplots}

There are many ways to examine how well a model fits the data, and this step is important in deciding whether the model is appropriate. Most diagnostics are based on the residuals, the difference between the $\hat{y}=\hat{\alpha}+\hat{\beta} x$ fitted values and the actual data points.

If needed, the fitted values and residuals can be extracted using `fitted(model)` and `residuals(model)` respectively.

The two simplest and most useful diagnostic plots are the scale-location plot and a QQ-plot of the residuals. These can be produced with `plot`, but we much prefer two functions from the `car` package, as shown by the following example (Fig.~\ref{fig:diagnos1}):
```{r diagnos1, fig.cap='Two standard diagnostic plots for a fitted lm object.', opts.label='wide', echo=-c(1:2)}
palette("default")
par(mfrow=c(1,2))
model <- lm(leafarea ~ diameter, data=allom)

library(car)
residualPlot(model)
qqPlot(model)
```

The scale-location plot shows  the square root of the *standardized* residuals against the fitted values. In an ideal situation, there should be no structure in this plot. Any curvature indicates that the model is under- or over-fitting, and a general spread-out (or contracting) from left to right indicates non-constant variance ('heteroscedasticity'). The QQ-plot enables us to check for departures from normality. Ideally, the standardized residuals should lie on a straight line.

Some departure from the straight line is to be expected though, even when the underlying distribution is really normal. The `qqPlot` function from the `car` package enhances the standard QQ-plot, by including a confidence interval (Fig~\ref{fig:qqplotcar}). In this case, there is some evidence of heteroscedasticity, and possibly curvature. 

The following code makes the QQ-plot and a plot of the data on a log-log scale (Fig.~\ref{fig:qqplotcar}).

```{r qqplotcar, fig.cap='A plot of the Allometry data on a log-log scale.', opts.label="smallsquare", echo=-c(1:2)}
palette("default")
par(mfrow=c(1,2))
library(car)
qqPlot(model)
plot(leafarea ~ diameter, data=allom, log="xy")
```


On a log-log scale, it looks like the variance is much more constant, and the relationship is more linear. So, we go ahead and refit the model to log-transformed variables. 

As we can see in Fig.~\ref{fig:qqplotcar}, the diagnostic plots look much better, except for a couple of points at the lower left corner of the QQ-plot. Notice that these outliers have been marked with their row number from the dataframe.

The following code produces Fig.~\ref{fig:diagnos2}, including diagnostic plots and a plot of the data with a regression line added. Note that the `abline` function will only work as intended (shown before) on the log-log plot if we use log to the base 10 (`log10`), in the model fit.

```{r diagnos2, fig.cap='Diagnostic plots for the allometry data, refitted on a log-log scale (left panels). The allometry data fitted on a log-log scale, with the regression line (right panel).', opts.label="extrawide", echo=-1}
par(mfrow=c(1,3))
model_log <- lm(log10(leafarea)~log10(diameter), data=allom)
summary(model_log)

residualPlot(model_log)
qqPlot(model_log)

plot(leafarea~diameter, data=allom, log="xy")
abline(model_log)
```


> **Try this yourself**
The residuals of a linear model fit can be extracted with the `residuals` function. For one of the linear models from the above examples, extract the residuals, and make a histogram to help inspect normality of the residuals.


