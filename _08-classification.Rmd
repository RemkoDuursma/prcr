# Classification

## Introduction

We have now explored various basic and more advanced regression approaches - each time we were trying to explain variation in a *continuous* (numeric) variable. In classification, we are instead trying to explain a *discrete* (factor) variable. There are many practical applications of classification, and R has many tools to fit, analyze, and visualize classification models.

In the previous chapter, we looked in detail at logistic regression models. These models aim to predict the *probability* of some occurrence (for example, the probability of dying when you travelled on the Titanic's ultimate journey). What if we want to convert a *probability* (a number between 0 and 1) to a *classifier* (either 0 *or* 1)? We will look at this problem in the next section.

After we know how to construct a classifier, we will look at a few different popular classification algorithms, without going into much of the theory underlying them. Instead, we will treat the topic by example.

**Packages used in this chapter**

```{r include=FALSE, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(ranger))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggthemes))
```

- `caret`
- `rpart`
- `ranger`



## From probabilities to classification

threshold
confusion matrix




```{r}
data(titanic)

library(rpart)

titanic$Survived <- as.factor(titanic$Survived)
r1_titanic <- rpart(Survived ~ PClass + Age + Sex, data=titanic)

plot(r1_titanic)

library(rpart.plot)
prp(r1_titanic,
    clip.right.labs = FALSE)

# this seems like a nice option
# box:
# - 0 or 1 (predicted class on this leaf)
# - classification rate (correct / n)
# - % of all observations fall into this leaf
xy <- rpart.plot(r1_titanic, type=3, extra=102)


caret::confusionMatrix(predict(r1_titanic, type="class"),
                       titanic$Survived,
                       positive = "1")


```




```{r eval=FALSE}
# Make ROC figure with two confusion matrices pointed out
# plot table on top of plot with plotrix::addtable2plot
```




```{r eval=FALSE}
library(dplyr)
data(cereals)
cereals <- read.csv("Cereals.csv") %>%
  filter(Manufacturer != "A") %>%
  droplevels


form <- formula(Manufacturer ~ calories + protein + fat + sodium + fiber + carbo + sugars + potass + vitamins)

library(rpart)
c1 <- rpart(form, data=cereals, method="class")

caret::confusionMatrix(predict(c1, type = "class"), 
                       cereals$Manufacturer)$table


library(randomForest)

# can omit NAs but sucks because we can't make the confusion matrix
c2 <- randomForest(form, data=cereals, mtry = 3, na.action = na.omit)
caret::confusionMatrix(predict(c2), 
                       cereals$Manufacturer)$table


cereals2 <- cereals[complete.cases(cereals),]
c2 <- randomForest(form, data=cereals2, mtry = 3, na.action = na.omit)
caret::confusionMatrix(predict(c2), 
                       cereals2$Manufacturer)

# Accuracy
sum(predict(c2) == cereals2$Manufacturer) / nrow(cereals2)



library(e1071)

c3 <- naiveBayes(form, data=cereals)
caret::confusionMatrix(predict(c3, newdata = cereals), 
                       cereals$Manufacturer)


library(AppliedPredictiveModeling)
transparentTheme(trans = .4)
library(caret)
featurePlot(x = cereals[, 4:12], 
            y = cereals$Manufacturer, 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 3))

```



## Visualizing classifiers


```{r eval=FALSE}


library(ggplot2)
library(ggthemes)


ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, col=Species)) +
  geom_point(size=3) +
  theme_tufte() +
  scale_colour_economist() +
  lims(x=c(4,8), y=c(2,4.5))


```




```{eval = FALSE}
class_predictions_plot <- function(model, n=101, fun=function(x)x, add_points=TRUE){
  
  out <- expand.grid(Sepal.Length = seq(4,8, length=n),
                     Sepal.Width = seq(2, 4.5, length=n))
  
  pred <- predict(model, newdata = out)
  out$pred_species <-  fun(pred)

  g <- ggplot(out, aes(x = Sepal.Length, y = Sepal.Width)) +
    geom_tile(aes(fill=pred_species)) +
    scale_fill_economist() +
    theme_tufte()
  
  if(add_points){
    g <- g + 
      geom_point(data = iris, 
                 aes(x = Sepal.Length, y = Sepal.Width, fill=Species),
                 pch = 21, size=2.5) 
  }
  
print(g)
}


library(MASS)
lda1 <- lda(Species ~ Sepal.Length + Sepal.Width, data=iris)

class_predictions_plot(lda1, fun = function(x)x$class)




library(randomForest)
rf1 <- randomForest(Species ~ Sepal.Length + Sepal.Width, data=iris)

class_predictions_plot(rf1)



library(rpart)
rp1 <- rpart(Species ~ Sepal.Length + Sepal.Width, data=iris)
class_predictions_plot(rp1)




library(caret)

rp1 <- train(Species ~ Sepal.Length + Sepal.Width, data=iris, method="rpart")
class_predictions_plot(rp1)


# 
m1 <- train(Species ~ Sepal.Length + Sepal.Width, data=iris, method="multinom")
class_predictions_plot(m1)

```



```{r eval=FALSE}
library(e1071)

s1 <- svm(Species ~ Sepal.Length + Sepal.Width, data=iris, kernel = "radial")
class_predictions_plot(s1)

s2 <- svm(Species ~ Sepal.Length + Sepal.Width, data=iris, kernel = "linear")
class_predictions_plot(s2)

s3 <- svm(Species ~ Sepal.Length + Sepal.Width, data=iris, kernel = "polynomial")
class_predictions_plot(s3)

s4 <- svm(Species ~ Sepal.Length + Sepal.Width, data=iris, kernel = "sigmoid")
class_predictions_plot(s4)


```

