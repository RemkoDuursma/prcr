
### Writing functions


1.  Write a function that adds two numbers, and divides the result by 2.

```{r }
addtwo <- function(num1, num2){
  (num1 + num2)/2
}
```

2.  You learned in Section \@ref(workingtext) that you can take subset of a string using the `substr` function. First, using that function to extract the first 2 characters of a bit of text. Then, write a function called `firstTwoChars` that extracts the first two characters of any bit of text.

```{r }
firstTwoChars <- function(txt){
  
  twoch <- substr(txt, 1, 2)
return(twoch)
}
```


3.  Write a function that checks if there are any missing values in a vector (using `is.na` and `any`). The function should return `TRUE` if there are missing values, and `FALSE` if not.

```{r }
anymiss <- function(x)any(is.na(x))
```


4.  Improve the function so that it tells you which of the values are missing, if any (*Hint:*use the `which` function). You can use `message` to write messages to the console.
```{r }
anymiss <- function(x){
  miss <- any(is.na(x))
  if(miss){
    message("The following observations are missing:")
    print(which(is.na(x)))
  }
return(miss)
}
```

5.  The function `readline` can be used to ask for data to be typed in. First, figure out how to use `readline` by reading the corresponding help file. Then, construct a function called `getAge` that asks the user to type his/her age. (*Hint:* check the examples in the `readline` help page).

```{r }
getAge <- function(){
  
  age <- readline("How old are you: ")
  
return(as.numeric(age))
}
```

6. **Hard**. Look at the calculations for a confidence interval of the mean in the example in Section \@ref(inference). Write a function that returns the confidence interval for a vector. The function should have two inputs: the vector, and the desired 'alpha'.

```{r }
calculateCI <- function(x, alpha=0.05){

  xbar <- mean(x)
  s <- sd(x)
  n <- length(x)
  half.width <- qt(1-alpha/2, n-1)*s/sqrt(n)
  
  # Confidence Interval 
  CI <- c(xbar - half.width, xbar + half.width)
  
return(CI)
}

calculateCI(rnorm(100))
```


7. **hard** Recall the functions `head` and `tail`. Write a function called `middle` that shows a few rows around (approx.) the 'middle' of the dataset. *Hint:* use `nrow`, `print`, and possibly `floor`.

```{r }
# Solution 1 : Shows ten rows starting at middle
middle <- function(x, n=10){
  
  m <- floor(nrow(x)/2)
  sub <- x[m:(m+n),]
  print(sub)
}

# Solution 2 : Shows ten rows AROUND middle
middle <- function(x, n=10){
  
  m <- floor(nrow(x)/2)
  start <- floor(m - n/2)
  end <- ceiling(m + n/2)-1
  
  sub <- x[start:end,]
  print(sub)
}
```




### Working with lists


First read the following list:

```
veclist <- list(x=1:5, y=2:6, z=3:7)
```

```{r echo=FALSE}
veclist <- list(x=1:5, y=2:6, z=3:7)
```



1.  Using `sapply`, check that all elements of the list are vectors of the same length. Also calculate the sum of each element. 

```{r }
sapply(veclist, length)

all(sapply(veclist,length) == 5)

sapply(veclist, sum)
```

2.  Add an element to the list called 'norms' that is a vector of 10 numbers drawn from the standard normal distribution (recall Section \@ref(distributions)).

```{r }
veclist$norms <- rnorm(10)
```

3.  Using the `pupae` data, use a $t$-test to find if PupalWeight varies with temperature treatment, separate for the two CO$_2$ treatments (so, do two $t$-tests). You **must** use `split` and `lapply`.
```{r }
data(pupae)
pupae_sp <- split(pupae, pupae$CO2_treatment)

lapply(pupae_sp, function(x)t.test(PupalWeight ~ T_treatment, data=x))
```


4.  For this exercise use the `coweeta` data - a dataset with measurements of tree size. Split the data by `species`, to produce a list called `coweeta_sp`. Keep only those species that have at least 10 observations. (*Hint:* first count the number of observations per species, save that as a vector, find which are at least 10, and use that to subscript the list.) If you don't know how to do this last step, skip it and continue to the next item.

```{r }
data(coweeta)

# Make list of dataframes
coweeta_sp <- split(coweeta, coweeta$species)

# Keep only species with at least 10 observations
# SOLUTION 1.
nspec <- sapply(coweeta_sp,nrow)  # count number of species
morethan10 <- nspec > 9   # logical vector, TRUE when nspec > 9

# Use that to index the list (with single square bracket!!!)
coweeta_sp <- coweeta_sp[morethan10]


# SOLUTION 2.
nspec <- sapply(coweeta_sp,nrow)
# find names of species that have at least 10 observations
morethan10 <- names(nspec)[nspec > 9] 

# Use that to subset the original data, and resplit
coweeta_subs <- droplevels(subset(coweeta, species %in% morethan10))
coweeta_sp <- split(coweeta_subs, coweeta_subs$species)


# SOLUTION 3 (dplyr)
library(dplyr)
coweeta_subs2 <- group_by(coweeta, species) %>%
  filter(n() >= 10)
  
```

5.  Using the split Coweeta data, perform a linear regression of `log10(biomass)` on `log10(height)`, separately by species. Use `lapply`.

```{r }
lms <- lapply(coweeta_sp, function(x)lm(log10(biomass) ~ log10(height),
                                        data=x))

```


### Functions for histograms


First run this code to produce two vectors.

```
x <- rnorm(100)
y <- x + rnorm(100)
```


1. Run a linear regression y = f(x), save the resulting object. Look at the structure of this object, and note the names of the elements. Extract the residuals and make a histogram.

```{r }
x <- rnorm(100)
y <- x + rnorm(100)

lmfit <- lm(y ~ x)
str(lmfit)
hist(lmfit$residuals)
```

2. **Hard**. From the previous question, write a function that takes an `lm` object as an argument, and plots a histogram of the residuals.

```{r }
# good:
lmResidHist <- function(lmobj){
  hist(lmobj$residuals)  
}

# better:
# You can now use any argument that hist() recognizes,
# they are passed through the use of '...'
lmResidHist <- function(lmobj,...){
  hist(lmobj$residuals,...)  
}

lmResidHist(lmfit, main="My Residuals")
```




### Using functions to make many plots



1.  Read the cereals data. Create a subset of data where the `Manufacturer` has at least two observations (use `table` to find out which you want to keep first). Don't forget to drop the empty factor level you may have created!

```{r }
data(cereals)
tab <- table(cereals$Manufacturer)
morethan1 <- names(tab)[tab > 1]
cereals <- droplevels(subset(cereals, Manufacturer %in% morethan1))
```

2.  Make a single PDF with six plots, with a scatter plot between potassium and fiber for each of the six (or seven?) Manufacturers. (*Hint:* ook at the template for producing a PDF with multiple pages at the bottom of Section \@ref(simpleloops).

```{r eval=FALSE}
cerealsp <- split(cereal, cereal$Manufacturer)

pdf("cereal plots.pdf", onefile=TRUE)
for(i in 1:length(cerealsp)){
  with(cerealsp[[i]],
       plot(potass, fiber, main=names(cerealsp)[i])
       )
}
dev.off()
```





### Monthly weather plots


1.  For the HFE weather dataset (`hfemet2008`) that makes a scatter plot between PAR (a measure of light intensity) and VPD (a measure of the dryness of air).

```{r }
# This function works for any dataframe, as long as it has
# columns names 'PAR' or 'VPD' in it.
PARVPD <- function(dat){
  with(dat, plot(PAR, VPD))
}
```

1.  Then, split the dataset by month (recall Section \@ref(dfrlists)), and make twelve such scatter plots. Save the result in a single PDF, or on one page with 12 small figures.
```{r }
data(hfemet2008)

library(lubridate)
hfemet2008$DateTime <- mdy_hm(as.character(hfemet2008$DateTime))

# extract month,
hfemet2008$month <- month(hfemet2008$DateTime)

hfesp <- split(hfemet2008, hfemet2008$month)

# windows(10,10)  # optional - add this command if the window is too small
par(mfrow=c(3,4))
for(i in 1:12)PARVPD(hfesp[[i]])

```





<!-- ### The Central limit theorem -->

<!-- The 'central limit theorem' (CLT) forms the backbone of inferential statistics. This theorem states (informally) that if you draw samples (of *n* units) from a population, the mean of these samples follows a normal distribution. This is true regardless of the underlying distribution you sample from.  -->

<!-- In this exercise, you will apply a simple simulation study to test the CLT, and to make histograms and quantile-quantile plots.  -->



<!-- 1. Draw 200 samples of size 10 from a uniform distribution. Use the `runif` function to sample from the uniform distribution, and the `replicate` function to repeat this many times. -->
<!-- ```{r } -->
<!-- unisamples <- replicate(200, runif(10)) -->
<!-- ``` -->

<!-- 1.  Compute the sample mean for each of the 200 samples in~\ref{ite:draw200}. Use `apply` or `colMeans` to calculate column-wise means of a matrix (note: `replicate` will return a matrix, if used correctly). -->
<!-- ```{r } -->
<!-- colMeans(unisamples) -->
<!-- ``` -->

<!-- 1.  Draw a histogram of the 200 sample means, using `hist`. Also draw a normal quantile-quantile plot, using `qqnorm`.  -->
<!-- ```{r } -->
<!-- hist(colMeans(unisamples)) -->
<!-- qqnorm(colMeans(unisamples)) -->
<!-- ``` -->


<!-- 1.  On the histogram, add a normal curve using the `dnorm` function. Note: to do this, plot the histogram with the argument `freq=FALSE`, so that the histogram draws the probability density, not the frequency.  -->
<!-- ```{r } -->
<!-- X <- colMeans(unisamples) -->
<!-- hist(X, freq=FALSE) -->
<!-- curve(dnorm(x, mean=mean(X), sd=sd(X)), add=T) -->
<!-- ``` -->

<!-- \item \hard Write a function that does all of the above, and call it `PlotCLT`. -->
<!-- ```{r } -->
<!-- plotCLT <- function(n1=200, n2=10){ -->
<!--   unisamples <- replicate(n1, runif(n2)) -->
<!--   X <- colMeans(unisamples) -->
<!--   hist(X, freq=FALSE) -->
<!--   curve(dnorm(x, mean=mean(X), sd=sd(X)), add=T) -->
<!-- } -->
<!-- ``` -->


