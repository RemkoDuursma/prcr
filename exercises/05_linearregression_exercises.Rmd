

### Probabilities

For this exercise, refer to the tables and examples in Section \@ref(distributions).


1.  For a normal random variable $X$ with mean 5.0, and standard deviation 2.0, find the probability that $X$ is less than 3.0.

```{r }
# pnorm gives the cumulative probability (i.e. prob. that value is less than some value).
pnorm(3.0, 5,2)
```

2.  Find the probability that $X$ is *greater than* 4.5.

```{r }
# Because pnorm gives the probability that X is *less than* some value,
# we need the inverse.

# Solution 1 : use 'lower.tail=FALSE'
pnorm(4.5, 5,2, lower.tail=FALSE)

# Solution 2 : calculate the complement (because total probability must be 1!)
1-pnorm(4.5,5,2)
```

3.  Find the value $K$ so that $P(X > K) = 0.05$.

```{r }
# qnorm is the opposite of pnorm. Given a probability, find K so that the probability
# of X is less than K is equal to that probability. Note that the default of qnorm
# is to find the probability *less than* some value. We therefore need the inverse.

# Because total probability is 1, 
# P(X > K) = 0.05 is the same as P(X < K) = 1-0.05
qnorm(0.95, 5, 2)

# Or, use lower.tail=FALSE
qnorm(0.05, 5, 2, lower.tail=FALSE)
```


4.  When tossing a fair coin 10 times, find the probability of seeing no heads (*Hint:* this is a binomial distribution).

```{r }
# dbinom finds the probability of 'x' occurrences (0 in this case) when we 
# repeat N ('size') events (here, 10), each with probability 'prob' (here, 0.5).
dbinom(x = 0, size = 10, prob = 0.5)
```

5.  Find the probability of seeing exactly 5 heads.

```{r }
# Same as before, but now K=5.
dbinom(x = 5, size = 10, prob = 0.5)
```

6.  Find the probability of seeing more than 7 heads.

```{r }
# Here we can use the cumulative probability, but realizing that the default gives us
# the probability of *less than* the given number of events.
1-pbinom(q=7, size=10, prob=0.5)
```




### Univariate distributions


1.  Simulate a sample of 100 random data points from a normal distribution with mean 100 and standard deviation 5, and store the result in a vector called `x`.
```{r }
# rnorm simulates from a normal distribution.
# Here we store the results in vector x.
x <- rnorm(n=100, mean=100, sd=5)
```

2.  Plot a histogram and a boxplot of the vector you just created (see Section\@ref(hist)). If you haven't make a boxplot, simply use the `boxplot` function on your vector!

```{r }
# This command puts the next two plots side-by-side
par(mfrow=c(1,2))
hist(x)
boxplot(x)
```

3.  Calculate the sample mean, standard deviation, median and interquartile range.
```{r }
mean(x)
sd(x)
```

4.  Using the data above, test the hypothesis that the mean equals 100 (using `t.test`). In science, it is customary (though debatable) to call an effect `significant` if the p-value is smaller than 0.05. Note that here we test whether the true mean is different from 100 - in this case we *know* the true mean since the data were sampled from a normal distribution with mean 100. **Bonus question**: how often do we find a p-value smaller than 0.05 in this example, do you think? (that is, resampling the data from step 1., and then testing).

```{r }
t.test(x, mu=100)
```

5.  Test the hypothesis that mean equals 90. 
```{r }
t.test(x, mu=90)
```

6.  Repeat the above two tests using a Wilcoxon signed rank test. Compare the p-values with those from the $t$-tests you just did.
```{r }
wilcox.test(x, mu=100)
wilcox.test(x, mu=90)
```




### More $t$-tests

For this question, use the `pupae` data.


1.  Use the `t.test` function to compare `PupalWeight` by `T_treatment`.
```{r }
data(pupae)

# When we use a formula in t.test, it will test the means of PupalWeight between the two
# levels of T_treatment. Note that this only works when the factor (on the right-hand side)
# contains exactly two levels.
t.test(PupalWeight ~ T_treatment, data=pupae,
       var.equal=TRUE)
```

2.  Repeat above using a Wilcoxon rank sum test. 
```{r }
wilcox.test(PupalWeight ~ T_treatment, data=pupae)
```

3.  Run the following code to generate some data:

```
base <- rnorm(20, 20, 5)
x <- base + rnorm(20,0,0.5)
y <- base + rnorm(20,1,0.5)
```

```{r echo=FALSE}
base <- rnorm(20, 20, 5)
x <- base + rnorm(20,0,0.5)
y <- base + rnorm(20,1,0.5)
```


4.  Using a two-sample t-test compare the means of `x` and `y`, assume that the variance is equal for the two samples.
```{r }
t.test(x,y, var.equal=TRUE)
```

5.  Repeat the above using a paired $t$-test. How has the $p$-value changed? 
```{r }
# The p-value is much smaller.
t.test(x,y, paired=TRUE)
```

6.  Which test is most appropriate?
```{r }
# The paired t-test is more appropriate because X and Y are 
# not independent : they use the same 'base' value.
```




### Simple linear regression

Continue with the `pupae` data. Perform a simple linear regression of `Frass` on `PupalWeight`. Produce and inspect the following:

1.  Summary of the model.
```{r }
model <- lm(Frass ~ PupalWeight, data = pupae)
summary(model)
```

2.  Diagnostic plots.
```{r }
# To place side-by-side
par(mfrow=c(1,2))
# QQ plot and residual plot. 
library(car)
residualPlot(model)
qqPlot(model)
```

3.  All of the above for a subset of the data, where `Gender` is 0, and `CO2\_treatment` is 400.
```{r }
# We can pass a subset directly to lm(). Alternatively, make the subset first with subset().
plot(Frass ~ PupalWeight, data = pupae, subset=Gender==0 & CO2_treatment == 400)
model <- lm(Frass ~ PupalWeight, data = pupae, subset=Gender==0 & CO2_treatment == 400)
summary(model)

par(mfrow=c(1,2))
library(car)
residualPlot(model)
qqPlot(model)
```



### Quantile Quest

You have already used quantile-quantile (QQ) plots many times, but in this exercise you will get to the bottom of the idea of comparing quantiles of distributions.

As in the previous exercises, we will use the `pupae` data.


1.  From the pupae data, extract `PupalWeight` and store it as a vector called 'pupweight'. Make a histogram of this vector (\@ref(hist)), noticing that the distribution seems perhaps quite like the normal distribution.

```{r }
pupweight <- pupae$PupalWeight
hist(pupweight)
```

2.  When we say 'quite like the normal distribution', we mean that the overall shape seems similar. Now simulate a histogram like the one above, using `rnorm` with the mean and standard deviation of the pupal weights (i.e. `pupweight`), and the same sample size as well. Plot it repeatedly to get an idea of whether the simulated histogram looks similar often enough.

```{r }
hist(rnorm(length(pupweight), mean=mean(pupweight), sd=sd(pupweight)))


# Better yet, place them side-by-side:
par(mfrow=c(1,2))
hist(pupweight)
hist(rnorm(length(pupweight), mean=mean(pupweight), sd=sd(pupweight)))
```

3.  Of course a visual comparison like that is not good enough, but it is a useful place to start. We can also compare the quantiles as follows. If we calculate the 25% and 75% quantiles of `pupweight`, we are looking for the values below which 25% or 75% of all observations occur. Clearly if two distributions have the same *shape*, their quantiles should be roughly similar. Calculate the 25, 50 and 75% quantiles for `pupweight`, and also calculate them for the normal distribution using `qnorm`. Are they similar?

```{r }
# Some quantiles of the measured distribution
quantile(pupweight, c(0.25, 0.5, 0.75))

# Some quantiles of the standard normal distribution with the same mean and sd:
qnorm(c(0.25, 0.5, 0.75), mean=mean(pupweight), sd=sd(pupweight))

# The values are very similar! We can conclude that for these quantiles at least, our data
# behaves as if they were drawn from a normal distribution.
```


4.  Now repeat the above exercise, but calculate many quantiles (e.g. from 2.5% to 97.5% with steps of 2.5% or whatever you choose) for both the measured data, and the standard normal distribution. Compare the two with a simple scatter plot, and add a 1:1 line. If you are able to do this, you just made your own QQ-plot (and if not, I suggest you inspect the solutions to this Exercise). *Hint:* use `seq` to make the vector of quantiles, and use it both in `quantile` and `qnorm`. Save the results of both those as vectors, and plot. As a comparison, use `qqPlot(pupweight, distribution="norm")` (`car` package), make sure to plot the normal quantiles on the X-axis.

```{r }
# Set up a vector of probabilities, used in calculating the quantiles.
qs <- seq(0.025, 0.975, by=0.025)

# Quantiles of the measured data
q_meas <- quantile(pupweight, probs=qs)

# Quantiles of the corresponding normal distribution
q_norm <- qnorm(qs, mean=mean(pupweight), sd=sd(pupweight))

# A simple 1:1 plot
plot(q_norm, q_meas)
abline(0,1)

# A standard QQ plot. Make sure to use the normal distribution because qqPlot uses
# the t-distribution by default.
# The two are not exactly the same because of the choice of the quantiles,
# but they are similar enough.
library(car)
qqPlot(pupweight, distribution="norm")
```



### One-way ANOVA


1.  For the `titanic` data, use a one-way ANOVA to compare the average passenger age by passenger class. (Note: by default, `lm` will delete all observations where `Age` is missing.)
```{r }
# Read data
data(titanic)

fit <- lm(Age~PClass, data=titanic)
summary(fit)
```


2.  For the Age and Memory data (Section\@ref(agemem}, p. sec:agemem}), make a subset of the `Older` subjects, and conduct a one-way ANOVA to compare words remembered by memory technique.
```{r }
data(memory)

memOlder <- subset(memory, Age=="Older")
fit <- lm(Words~Process, data=memOlder)

summary(fit)
```



### Two-way ANOVA


1.  Using the pupae dataset, fit a two-way ANOVA to compare `PupalWeight` to `Gender` and `CO2_treatment`. Which main effects are significant? After reading in the pupae data, make sure to convert `Gender` and `CO2_treatment` to a factor first (see Section\@ref(workingfactors)).
```{r }
data(pupae)
pupae$CO2_treatment <- as.factor(pupae$CO2_treatment)

fit <- lm(PupalWeight ~ Gender+CO2_treatment, data=pupae)
summary(fit)

library(car)
Anova(fit)
```


2.  Is there an interaction between `Gender` and `CO2_treatment`?
```{r }
# To test the interaction, add all terms (using *), and inspect the p-value for the interaction 
# term in the ANOVA table.
fit <- lm(PupalWeight ~ Gender*CO2_treatment, data=pupae)
Anova(fit)
```


3.  Repeat the above using `T_treatment` instead of `CO2_treatment`.
```{r }
fit <- lm(PupalWeight ~ Gender+T_treatment, data=pupae)
summary(fit)
Anova(fit)

fit2 <- lm(PupalWeight ~ Gender*T_treatment, data=pupae)
Anova(fit2)
```






### Regression: the `pulse` dataset {#multregexerc}

The `pulse` data contains measurements of heart rate ("pulse") for individuals before and after running (and some control individuals) - and the individuals' height, weight and age (and other interesting variables). The `Pulse1` variable is the resting heart rate before any treatment, and `Pulse2` the heart rate after the treatment (half the subjects engaged in running). 

1.   Read the data and fit a multiple linear regression of `Pulse1` against `Age`, `Weight` and `Height` (add the variables to the model in that order). Are any terms significant at the 5\% level? What is the R^2^?

```{r }
data(pulse)

# Fit model (without interactions)
pulse_fit <- lm(Pulse1 ~ Age+Weight+Height, data=pulse)
summary(pulse_fit)

library(car)
Anova(pulse_fit)

# Plot model diagnostics
par(mfrow=c(1,2))
residualPlot(pulse_fit)
qqPlot(pulse_fit)
```


2.  Now also include the factor `Exercise` in the regression model (an indicator of whether the subject exercises frequently or not). You will need to first convert `Exercise` to a factor as it is stored numerically in the data. Does adding `Exercise` improve the model?

```{r }
pulse$Exercise <- as.factor(pulse$Exercise)
pulse_fit2 <- lm(Pulse1 ~ Age+Weight+Height+Exercise, data=pulse)
summary(pulse_fit2)
Anova(pulse_fit2)

# AIC shows only a very marginal improvement.
AIC(pulse_fit, pulse_fit2)
```


3.  Using the same  data, fit a model of `Pulse2` as a function of `Pulse1` and `Ran` as main effects only (Note: convert `Ran` to a factor first). Use the `visreg` package to understand the fit.


```{r }
pulse$Ran <- as.factor(pulse$Ran)
pulse_fit3 <- lm(Pulse2~Pulse1+Ran, data=pulse)
Anova(pulse_fit3) 

# It is important that you visualize this model,
# since it has a numeric and a factor variable.
library(visreg)
visreg(pulse_fit3, "Pulse1", by="Ran", overlay = TRUE)

# It appears that the effect of running is a constant increase in heart rate - 
# independent of the resting heart rate (since the lines are practically parallel).
```


4.  Now add the interaction between `Pulse1` and `Ran`. Is it significant? Also look at the effects plot, how is it different from the model without an interaction?

```{r }
pulse_fit4 <- lm(Pulse2~Pulse1*Ran, data=pulse)
Anova(pulse_fit4) 

visreg(pulse_fit4, "Pulse1", by="Ran", overlay = TRUE)
```



5. Read Section \@ref(importance) on variable importance. Add a new variable to the `pulse` dataset, as the difference between `Pulse2` and `Pulse1` (the change in heartrate after exercise). Now fit a linear regression model with all of these terms: "Height", "Weight", "Age", "Gender", "Smokes", "Alcohol","Exercise","Ran". As expected `Ran` wil be most important, but how do all the other variables rank in importance?

```{r}
pulse$change_pulse <- pulse$Pulse2 - pulse$Pulse1

fit_pulse <- lm(change_pulse ~ Height + Weight + Age + Gender + Smokes + Alcohol + Exercise + Ran,
                data=pulse)

library(relaimpo)
fit_pulse_importance <- calc.relimp(fit_pulse, type="lmg")
sort(fit_pulse_importance$lmg, TRUE)

```






### Logistic regression


1.  Using the Pulse data once more, build a model to see if `Pulse2` can predict whether people were in the `Ran` group. Make sure that `Ran` is coded as a factor.

```{r }
# This is a logistic regression, since Ran takes only two possible values.
fit6 <- glm(Ran ~ Pulse2, data=pulse, family=binomial)
summary(fit6)
```

1.  The `visreg` package is very helpful in visualizing the fitted model. For the logistic regression you just fit , run the following code and make sure you understand the output. (This code assumes you called the object `fit6`, if not change `fit6` to the name you used.)
\begin{verbatim}
library(visreg)
visreg(fit6, scale="response")
\end{verbatim}
```{r }
# This plot shows the fitted logistic regression (with an approximate 
# confidence interval).
# When pulse was low, the probability that the subject was in the 'Ran' group
# is very close to 1.
library(visreg)
visreg(fit6, scale="response")
```




<!-- ### Generalized linear model (GLM) -->


<!-- \item First run the following code to generate some data, -->
<!-- \begin{verbatim} -->
<!-- len <- 21 -->
<!-- x <- seq(0,1,length=len) -->
<!-- y <- rpois(len,exp(x-0.5)) -->
<!-- \end{verbatim} -->
<!-- ```{r echo=FALSE} -->
<!-- len <- 21 -->
<!-- x <- seq(0,1,length=len) -->
<!-- y <- rpois(len,exp(x-0.5)) -->
<!-- ``` -->

<!-- 1.  Fit a Poisson GLM to model `y` on `x`. Is `x` significant in the model? -->
<!-- ```{r } -->
<!-- fit7 <- glm(y ~ x, family=poisson) -->
<!-- summary(fit7) -->
<!-- ``` -->

<!-- 1.  Repeat above with a larger sample size (e.g., `len <- 101`). Compare the results. -->
<!-- ```{r } -->
<!-- len <-101 -->
<!-- x <- seq(0, 1, length=len) -->
<!-- y <- rpois(len, exp(x-0.5)) -->

<!-- fit <-glm(y ~ x, family=poisson) -->
<!-- summary(fit) -->
<!-- ``` -->

<!-- 1.  The `memory` data were analysed assuming a normal error distribution in Section\@ref(twoway} and using a Poisson error distribution in Section\@ref(glm}, and each approach resulted in a different outcome for the significance of the interaction term. The participants in the study were asked to remember up to 27 words, not an unlimited number, and some of the participants were remembering close to this upper limit. Therefore, it may make sense to think of the response as consisting of a number of 'successes' and a number of 'failures', as we do in a logistic regression. Use `glm` to model the response using a binomial error distribution. Refer to Section\@ref(tabuldataglm} (p. sec:tabuldataglm}) for a similar example. -->

<!-- ```{r } -->
<!-- # read in data -->
<!-- memory <- read.delim('eysenck.txt') -->

<!-- # Fit glm model, with response represented by two columns:  -->
<!-- # number of words remembered and not remembered, or -->
<!-- # 'successes' and 'failures'. -->
<!-- m1 <- glm(cbind(Words, 27-Words) ~ Age * Process, data=memory, family=binomial) -->

<!-- # estimate significance of the main effects -->
<!-- Anova(m1) -->
<!-- ``` -->






